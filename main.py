from flask import Flask,request,jsonify
# -*- coding: utf-8 -*-
"""MiniProjDescAns

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kl2Amly-sRa7CiMIXzypVRka_S4cPUW2

# **ARNAV**

## Importing Libraries
"""

import pickle
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import nltk
import ssl
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

#nltk.download()
nltk.download('stopwords',quiet=True)
nltk.download('wordnet',quiet=True)
nltk.download('punkt',quiet=True)
nltk.download('omw-1.4',quiet=True)

#!pip --quiet install -U sentence-transformers
from sentence_transformers import SentenceTransformer,CrossEncoder

#!pip install transformers

from transformers import pipeline

#!python -m spacy download en_core_web_lg
import spacy
nlp = spacy.load("en_core_web_lg")

"""## Cosine-Similarity Function - cos_sim(emb1,emb2)
### Calculates cos-sim between 2 columns of embeddings
"""

def cos_sim(sentence1_emb, sentence2_emb):
    cos_sim = cosine_similarity(sentence1_emb, sentence2_emb)
    return np.diag(cos_sim)

"""## S-BERT CrossEncoder
### sbert_cross(answer, expected_answer)
"""

def sbert_cross(answer, expected_answer):
    # Load the model
    # sbert_cross_model = CrossEncoder('semantic_model')
    sbert_cross_model = CrossEncoder('cross-encoder/stsb-roberta-base')
    similarity = sbert_cross_model.predict([answer,expected_answer])
    return similarity

"""# **PRATHAM**

## Import the saved model and testing it

For running on our local machine(even CPU), we can just import the saved model and play with it
"""

# obtained_model_roberta=pipeline("text-classification",model="grammer_model")
obtained_model_roberta=pipeline("text-classification",model="imohammad12/GRS-Grammar-Checker-DeBerta")

"""## RoBERTa"""

def roberta(sentence):  
    o=obtained_model_roberta(sentence)
    ans=o[0]["score"]
    return ans

"""# **HAARISH**"""

nlp = spacy.load("en_core_web_lg")

def extract_POS(sample_doc):
    res=[]
    for chk in sample_doc.noun_chunks:
        tmp=""
        for tkn in chk:
            if (tkn.pos_ in ['NOUN','PROPN','ADJ'] ):
                if (not(tkn.is_stop) and not(tkn.is_punct)):
                    tmp = tmp + tkn.text.lower() + " "
        if(tmp.strip()!=""):
            res.append(tmp.strip())
    return list(dict.fromkeys(res))

def matching_keywords(stdlst,keylst):
        #matched list
        res=[]
        #unmatched list
        tmpres=[]
        for x in stdlst:
            if (x in keylst):
                res.append(x)
        return res

def dictionary_with_weights(words):
  
  # nouns are given weightage as 9
  # proper nouns are given weightage as 8.5
  # adjectives are given weightage as 5

  categorized_words = {'9': [], '8.5': [], '5': []}
  for word in words:
      doc = nlp(word)
      pos = doc[0].pos_
      if pos in ['NOUN', 'PRON']:
          categorized_words['9'].append(word)
      elif pos == 'PROPN':
          categorized_words['8.5'].append(word)
      elif pos == 'ADJ':
          categorized_words['5'].append(word)
  return categorized_words

#@title Final Scoring

def keyword_scoring(stud_ans,ans_key):

  ans_doc=nlp(ans_key)
  key_POS=extract_POS(ans_doc)
  

  model = SentenceTransformer('distilbert-base-nli-mean-tokens')
  doc_embedding = model.encode([ans_key])
  candidate_embeddings = model.encode(key_POS)
  top_n = 7
  distances = cosine_similarity(doc_embedding, candidate_embeddings)
  keywords = [key_POS[index] for index in distances.argsort()[0][-top_n:]]
  keywords_scores = dictionary_with_weights(keywords)
  

  total=0
  count=0

  for i in keywords_scores['9']:
    count+=1
  total=total+count*9

  count=0
  for i in keywords_scores['8.5']:
    count+=1
  total=total+count*8.5

  count=0
  for i in keywords_scores['5']:
    count+=1
  total=total+count*5

  
  if(type(stud_ans)==float):
    stud_ans=str(stud_ans)


  stud_doc=nlp(stud_ans) 
  std_POS=extract_POS(stud_doc)
  #then apply match function
  matched=matching_keywords(std_POS,keywords)
  stud_score=0
  #percentage of matched keywords along with weights
  for i in matched:
    if i in keywords_scores['9']:
      stud_score=stud_score+9
    elif i in keywords_scores['8.5']:
      stud_score=stud_score+8.5
    elif i in keywords_scores['5']:
      stud_score=stud_score+5 
  score=stud_score/total
  return score

"""# **Using Regression Model from saved file**"""

def finalMarks(li):
    loaded_model = pickle.load(open("regression.pickle",'rb')) 
    x=loaded_model.predict([li])
    np.floor(x)
    x=x.clip(0,7)
    x=int(x)
    return x

"""# **Final Marks**"""

def descAnswerEval(ans,key):
    return finalMarks([sbert_cross(ans,key),roberta(ans),keyword_scoring(ans,key)])


app = Flask(__name__)

@app.route('/', methods=['GET'])
def home():
    expected = request.args.get('expected')
    answer = request.args.get('answer')
    return jsonify({'score':str(descAnswerEval(expected,answer))})



if (__name__ == '__main__'):
    app.run(debug=False,host='0.0.0.0')
